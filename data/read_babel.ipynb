{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41038b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np, pathlib, torch\n",
    "from tqdm import tqdm\n",
    "babel_ann = json.load(open('babel_v1.0_release/train.json'))\n",
    "amass_root = pathlib.Path('amass')\n",
    "seq_info  = babel_ann['9864']          # 举例\n",
    "npz_file  = amass_root / seq_info['feat_p']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e673fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6615,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "babel_ann.__len__(),#(6615,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6007dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdata    = np.load(npz_file, allow_pickle=True)\n",
    "poses  = bdata ['poses']      # (T, 156)  axis-angle，每3维=1关节\n",
    "trans  = bdata ['trans']      # (T, 3)    根平移\n",
    "betas  = bdata ['betas'][None]# (1, 10)   身体形状\n",
    "gender = bdata ['gender'].item() if 'gender' in bdata  else 'neutral'\n",
    "T, D   = poses.shape\n",
    "J      = D // 3             # 关节数(52 for SMPL-H, 24 for SMPL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd5a7b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3157, 156),)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poses.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9488e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.transforms import axis_angle_to_matrix\n",
    "pose_aa  = torch.from_numpy(poses).float().reshape(-1, 3)       # (T·J, 3)\n",
    "rot_mats = axis_angle_to_matrix(pose_aa).reshape(T, J, 3, 3)    # (T, J, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a596c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "\n",
    "model_path = f'./amass/smplh/{gender.lower()}/model.npz'\n",
    "device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "body_model = BodyModel(\n",
    "        bm_fname=model_path,\n",
    "        num_betas=16, # 通常 AMASS 用 10 或 16 个 betas\n",
    "        num_dmpls=None, # 如果 AMASS 数据不包含 DMPLs\n",
    "        model_type='smplh' # 明确指定模型类型\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8bcf3550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择一帧进行演示\n",
    "frame_idx = 0 # 或者任意你感兴趣的帧\n",
    "num_frames = bdata['poses'].shape[0]\n",
    "if frame_idx >= num_frames:\n",
    "    print(f\"错误: frame_idx ({frame_idx}) 超出范围 (0-{num_frames-1})。\")\n",
    "    exit()\n",
    "\n",
    "all_poses_frame = torch.tensor(bdata['poses'][frame_idx:frame_idx+1], dtype=torch.float32).to(device) # (1, 156)\n",
    "root_orient = all_poses_frame[:, :3]        # 全局旋转 (轴角)\n",
    "pose_body = all_poses_frame[:, 3:66]       # 身体关节姿态 (21 joints * 3 = 63 params for SMPL-H)\n",
    "pose_hand = all_poses_frame[:, 66:66+90]   # 双手姿态 (15+15 joints * 3 = 90 params for SMPL-H)\n",
    "# 如果是 SMPL-X，还会有 pose_jaw, pose_eye 等\n",
    "\n",
    "betas = torch.tensor(bdata['betas'][:16], dtype=torch.float32).unsqueeze(0).to(device) # (1, 10)\n",
    "trans = torch.tensor(bdata['trans'][frame_idx:frame_idx+1], dtype=torch.float32).to(device) # (1, 3)\n",
    "\n",
    "# --- 2. & 3. 传递参数给模型并进行正向运动学 ---\n",
    "body = body_model(\n",
    "    root_orient=root_orient,\n",
    "    pose_body=pose_body,\n",
    "    # pose_hand=pose_hand, # 如果模型是 SMPL-H/X 且有手部参数\n",
    "    # pose_jaw=pose_jaw, # for SMPL-X\n",
    "    # pose_eye=pose_eye, # for SMPL-X\n",
    "    betas=betas,\n",
    "    trans=trans\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f035c68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 3)\n",
      "\n",
      "计算得到的第 0 帧的三维关节坐标 (部分示例):\n",
      "[[-0.15769717  0.04982522  0.7323806 ]\n",
      " [-0.14868572  0.12678236  0.64955086]\n",
      " [-0.19523332 -0.00525749  0.64258844]\n",
      " [-0.16806692  0.04579143  0.83666056]\n",
      " [ 0.08590449  0.06953263  0.41320103]]\n"
     ]
    }
   ],
   "source": [
    "# --- 4. 获取三维关节 ---\n",
    "# body 对象包含了很多信息，例如：\n",
    "# body.v: 模型的顶点 (1, num_vertices, 3)\n",
    "# body.Jtr: 三维关节位置 (1, num_joints, 3)\n",
    "# body.f: 模型的面片\n",
    "\n",
    "joints_3d = body.Jtr.detach().cpu().numpy().squeeze() # (num_joints, 3)\n",
    "print(joints_3d.shape) # 输出关节坐标的形状\n",
    "print(f\"\\n计算得到的第 {frame_idx} 帧的三维关节坐标 (部分示例):\")\n",
    "\n",
    "print(joints_3d[:5, :]) # 打印前5个关节的坐标\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "119b004a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3157, 52, 3, 3]), torch.float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from human_body_prior.tools.rotation_tools import aa2matrot\n",
    "#将pose 从轴角转换为旋转矩阵\n",
    "pose_aa = torch.tensor(bdata['poses'], dtype=torch.float32).reshape(-1, 3)  # (T·J, 3)\n",
    "rot_mats = aa2matrot(pose_aa).reshape(T, J, 3, 3)  # (T, J, 3, 3)\n",
    "rot_mats.shape, rot_mats.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6de595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将所有帧的轴角转为旋转矩阵，再转为6d旋转表征，\n",
    "# 然后再将6d旋转表征转换为轴角，输入到body_model中，查看前后的3D坐标误差\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e87ac2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch3D for rotation conversions.\n",
      "Calculating 3D joints with original poses...\n",
      "Original 3D joints shape: torch.Size([3157, 52, 3])\n"
     ]
    }
   ],
   "source": [
    "# --- Make sure these PyTorch3D utilities are available ---\n",
    "# (You might have imported axis_angle_to_matrix in cell [19] already)\n",
    "try:\n",
    "    from pytorch3d.transforms import (\n",
    "        axis_angle_to_matrix,\n",
    "        matrix_to_rotation_6d,\n",
    "        rotation_6d_to_matrix,\n",
    "        matrix_to_axis_angle\n",
    "    )\n",
    "    print(\"Using PyTorch3D for rotation conversions.\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch3D not found. Please install it to proceed with these conversions.\")\n",
    "    print(\"Installation: https://github.com/facebookresearch/pytorch3d/blob/main/INSTALL.md\")\n",
    "    # You might want to raise an error or use alternative functions if PyTorch3D is not available\n",
    "    raise\n",
    "\n",
    "\n",
    "# --- Prepare AMASS data for BodyModel (all frames) ---\n",
    "# Assuming 'poses_np', 'trans_np', 'bdata', 'device', 'body_model' are from previous cells\n",
    "# poses_np has shape (T, 156)\n",
    "# trans_np has shape (T, 3)\n",
    "# bdata['betas'] should provide shape parameters\n",
    "poses_np  = bdata ['poses']\n",
    "trans_np=trans  = bdata ['trans']      # (T, 3)    根平移\n",
    "T = poses_np.shape[0]\n",
    "num_pose_dims = poses_np.shape[1] # Should be 156 for SMPL-H\n",
    "\n",
    "num_betas_to_use = 16 # IMPORTANT: Match this with body_model init and your data\n",
    "betas_torch = torch.from_numpy(bdata['betas'][:num_betas_to_use]).float().unsqueeze(0).to(device) # Shape: (1, num_betas_to_use)\n",
    "# BodyModel will broadcast betas from (1, N) to (T, N) if T > 1\n",
    "\n",
    "trans_torch = torch.from_numpy(trans_np).float().to(device) # Shape: (T, 3)\n",
    "poses_torch_orig = torch.from_numpy(poses_np).float().to(device) # Shape: (T, 156)\n",
    "\n",
    "# Split original poses for body_model input\n",
    "root_orient_torch_orig = poses_torch_orig[:, :3]         # (T, 3)\n",
    "pose_body_torch_orig   = poses_torch_orig[:, 3:66]        # (T, 63) ; 21 body joints\n",
    "pose_hand_torch_orig   = poses_torch_orig[:, 66:156]      # (T, 90) ; 2x15 hand joints\n",
    "\n",
    "# --- Get 3D joint coordinates using ORIGINAL poses ---\n",
    "print(\"Calculating 3D joints with original poses...\")\n",
    "with torch.no_grad(): # No need to compute gradients for forward pass\n",
    "    body_model_orig_output = body_model(\n",
    "        root_orient=root_orient_torch_orig,\n",
    "        pose_body=pose_body_torch_orig,\n",
    "        pose_hand=pose_hand_torch_orig,\n",
    "        betas=betas_torch,\n",
    "        trans=trans_torch\n",
    "    )\n",
    "joints_3d_orig = body_model_orig_output.Jtr # (T, num_joints_output_by_body_model, 3)\n",
    "print(f\"Original 3D joints shape: {joints_3d_orig.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8bedd26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing rotation conversions...\n",
      "Rotation conversions completed.\n"
     ]
    }
   ],
   "source": [
    "# --- Perform rotation conversions: aa -> rotmat -> 6d -> rotmat -> aa ---\n",
    "print(\"Performing rotation conversions...\")\n",
    "\n",
    "# Reshape poses for batch conversion: (T, 156) -> (T * 52, 3)\n",
    "num_joints_smplh = num_pose_dims // 3 # 52 for SMPL-H\n",
    "poses_aa_flat_orig = poses_torch_orig.reshape(T * num_joints_smplh, 3)\n",
    "\n",
    "# 1. Axis-angle to Rotation Matrix\n",
    "rot_mats_flat = axis_angle_to_matrix(poses_aa_flat_orig) # (T * 52, 3, 3)\n",
    "\n",
    "# 2. Rotation Matrix to 6D Representation\n",
    "poses_6d_flat = matrix_to_rotation_6d(rot_mats_flat) # (T * 52, 6)\n",
    "\n",
    "# 3. 6D Representation back to Rotation Matrix\n",
    "rot_mats_flat_reconstructed = rotation_6d_to_matrix(poses_6d_flat) # (T * 52, 3, 3)\n",
    "\n",
    "# 4. Rotation Matrix back to Axis-angle\n",
    "poses_aa_flat_reconstructed = matrix_to_axis_angle(rot_mats_flat_reconstructed) # (T * 52, 3)\n",
    "\n",
    "# Reshape reconstructed poses back to (T, 156) for body_model\n",
    "poses_torch_reconstructed = poses_aa_flat_reconstructed.reshape(T, num_pose_dims)\n",
    "print(\"Rotation conversions completed.\")\n",
    "\n",
    "# Optional: Check intermediate shapes\n",
    "# print(f\"poses_aa_flat_orig shape: {poses_aa_flat_orig.shape}\")\n",
    "# print(f\"rot_mats_flat shape: {rot_mats_flat.shape}\")\n",
    "# print(f\"poses_6d_flat shape: {poses_6d_flat.shape}\")\n",
    "# print(f\"rot_mats_flat_reconstructed shape: {rot_mats_flat_reconstructed.shape}\")\n",
    "# print(f\"poses_aa_flat_reconstructed shape: {poses_aa_flat_reconstructed.shape}\")\n",
    "# print(f\"poses_torch_reconstructed shape: {poses_torch_reconstructed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9a2759bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating 3D joints with reconstructed poses...\n",
      "Reconstructed 3D joints shape: torch.Size([3157, 52, 3])\n",
      "\n",
      "--- Reconstruction Error Report ---\n",
      "Mean MPJPE over the sequence: 0.000000091 (units of your 3D coordinates, likely meters)\n",
      "Max single joint error in any frame: 0.000000588\n",
      "MPJPE for the first frame (frame 0): 0.000000071\n",
      "Mean L2 error between original and reconstructed axis-angle poses (per frame avg): 0.000000493\n"
     ]
    }
   ],
   "source": [
    "# --- Split reconstructed poses for body_model input ---\n",
    "root_orient_torch_recon = poses_torch_reconstructed[:, :3]\n",
    "pose_body_torch_recon   = poses_torch_reconstructed[:, 3:66]\n",
    "pose_hand_torch_recon   = poses_torch_reconstructed[:, 66:156]\n",
    "\n",
    "# --- Get 3D joint coordinates using RECONSTRUCTED poses ---\n",
    "print(\"Calculating 3D joints with reconstructed poses...\")\n",
    "with torch.no_grad():\n",
    "    body_model_recon_output = body_model(\n",
    "        root_orient=root_orient_torch_recon,\n",
    "        pose_body=pose_body_torch_recon,\n",
    "        pose_hand=pose_hand_torch_recon,\n",
    "        betas=betas_torch,        # Same betas\n",
    "        trans=trans_torch         # Same trans\n",
    "    )\n",
    "joints_3d_recon = body_model_recon_output.Jtr # (T, num_joints_output_by_body_model, 3)\n",
    "print(f\"Reconstructed 3D joints shape: {joints_3d_recon.shape}\")\n",
    "\n",
    "# --- Calculate and Display Error ---\n",
    "# Ensure shapes are compatible\n",
    "if joints_3d_orig.shape != joints_3d_recon.shape:\n",
    "    print(\"Error: Original and reconstructed 3D joint shapes do not match!\")\n",
    "    print(f\"Original shape: {joints_3d_orig.shape}, Reconstructed shape: {joints_3d_recon.shape}\")\n",
    "else:\n",
    "    # L2 norm for each joint, each frame (Euclidean distance)\n",
    "    error_per_joint_per_frame = torch.norm(joints_3d_orig - joints_3d_recon, dim=2) # (T, num_joints)\n",
    "\n",
    "    # Mean Per Joint Position Error (MPJPE) per frame\n",
    "    mpjpe_per_frame = error_per_joint_per_frame.mean(dim=1) # (T)\n",
    "\n",
    "    # Overall MPJPE for the sequence (mean over all frames)\n",
    "    mean_mpjpe_sequence = mpjpe_per_frame.mean().item()\n",
    "\n",
    "    # Max error to see worst-case reconstruction for a joint in a frame\n",
    "    max_error = error_per_joint_per_frame.max().item()\n",
    "\n",
    "    print(f\"\\n--- Reconstruction Error Report ---\")\n",
    "    print(f\"Mean MPJPE over the sequence: {mean_mpjpe_sequence:.9f} (units of your 3D coordinates, likely meters)\")\n",
    "    print(f\"Max single joint error in any frame: {max_error:.9f}\")\n",
    "\n",
    "    # Optionally, print error for the first frame's joints\n",
    "    if T > 0:\n",
    "        print(f\"MPJPE for the first frame (frame 0): {mpjpe_per_frame[0].item():.9f}\")\n",
    "        # print(\"Error for each joint in the first frame (frame 0):\")\n",
    "        # print(error_per_joint_per_frame[0].cpu().numpy())\n",
    "\n",
    "    # You can also compare the pose parameters themselves\n",
    "    pose_reconstruction_error = torch.norm(poses_torch_orig - poses_torch_reconstructed, dim=1).mean().item()\n",
    "    print(f\"Mean L2 error between original and reconstructed axis-angle poses (per frame avg): {pose_reconstruction_error:.9f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Alphapose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
